#!/bin/bash
{% if host == "janus" %}#SBATCH -J {{ J }}
#SBATCH --time={{ hours }}:00:00
#SBATCH -N {{ nodes }}
#SBATCH --ntasks-per-node {{ nntasks_per_node }}
#SBATCH -o {{ logname }}-%j.out
#SBATCH -e {{ logname }}-%j.err
#SBATCH --qos=normal

# Load important modules and ensure environemnt variables are correctly set up

module load python/anaconda-2.0.0
module load fftw/fftw-3.3.3_openmpi-1.4.5_intel-12.1.0_double_ib
module load intel/intel-12.1.6;
module load openmpi/openmpi-1.4.5_intel-12.1.6_ib;
export OMP_NUM_THREADS=1

{% elif host == "psiops" %}#PBS -l nodes={{ nodes }}:ppn={{ nntasks_per_node }}:{{ connection }} -q batch -l walltime={{ hours }}:00:00
#PBS -k eo -m n -e $PBS_O_WORKDIR/{{ logname }}-$PBS_JOBID.out -o $PBS_O_WORKDIR/{{ logname }}-$PBS_JOBID.out

# Header for script adds nodes list from PBS
cd $PBS_O_WORKDIR
NODES=`cat $PBS_NODEFILE | tr "\n" " "`
for node in $NODES; do
     echo $node >> $PBS_O_WORKDIR/machines
done

echo "PBS_JOBID: $PBS_JOBID" 1>>$PBS_O_WORKDIR/qperr
echo "Nodes: $NODES" 1>>$PBS_O_WORKDIR/qperr
echo "USER: $USER" 1>>$PBS_O_WORKDIR/qperr
echo "PBS_O_WORKDIR: $PBS_O_WORKDIR" 1>>$PBS_O_WORKDIR/qperr

# Get start time
stime=`date +%s`

# This sets environment variables
PATH=/home/apps/anaconda/2.3.0/bin/:/usr/lib64/qt-3.3/bin:/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/X11R6/bin:/root/bin:/usr/share/pvm3/lib:/usr/share/pvm3/lib:/opt/intel/Compiler/11.1/064/bin/intel64/:/home/dummy/open_mpi_intel/openmpi-1.6/bin
export PATH
LD_LIBRARY_PATH=/opt/intel/Compiler/11.1/064/lib/intel64/:/home/dummy/open_mpi_intel/openmpi-1.6/lib:/opt/intel/Compiler/11.1/064/mkl/lib/em64t
export LD_LIBRARY_PATH
PYTHONPATH=~/usr/bin/VTST-Tools:$PYTHONPATH

{% elif host == "rapunzel" %}#SBATCH --job-name=$3
#SBATCH --time={{ hours }}:00:00
#SBATCH -N {{ nodes }}
#SBATCH --ntasks-per-node {{ nntasks_per_node }}
#SBATCH -o {{ logname }}-%j.out
#SBATCH -e {{ logname }}-%j.err

LD_LIBRARY_PATH=/export/home/apps/compile/gcc.5.1.0/lib64:/opt/openmpi/lib:/opt/python/lib
export OMP_NUM_THREADS=1
{% endif %}


0
# Execute python commands, could be separated into a separate file, but 1 file is easier to template.

python -c "

from custodian.vasp.jobs import *
from custodian.vasp.handlers import *
from custodian.custodian import *
from Classes_Custodian import *

vaspjob = [{{ jobtype }}Job(['{{ mpi }}', '-npernode', '{{ nntasks_per_node }}', '{{ vasp_tst_kpts }}'], '{{ logname }}',
                            gamma_vasp_cmd=['{{ mpi }}', '-npernode', '{{ nntasks_per_node }}', '{{ vasp_tst_gamma }}'], auto_npar=False)]

{% if jobtype == "NEB" %}
handlers = [WalltimeHandler({{ hours }}*60*60, 15*60), NEBNotTerminating('{{ logname }}', 20*60)]

{% elif jobtype == "Dimer" %}
handlers = [WalltimeHandler({{ hours }}*60*60, 15*60), NEBNotTerminating('{{ logname }}', 20*60), PositiveEnergyErrorHandler(),
            PotimErrorHandler(), VaspErrorHandler('{{ logname }}')]

{% elif jobtype == "Standard" %}
handlers = [WalltimeHandler({{ hours }}*60*60), NonConvergingErrorHandler(nionic_steps=15), PositiveEnergyErrorHandler(),
            PotimErrorHandler(), VaspErrorHandler('{{ logname }}')]
{% endif %}

c = Custodian(handlers, vaspjob, max_errors=10)
c.run()"
